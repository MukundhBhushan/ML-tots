install with pip
```bash
pip install tensorflow
```

## Weights and biases
y=mx+c  
waight = m  
bias = c

## hyper parameter
a hyperparameter is a parameter whose value is set before the learning process begins and are changed during the course of the training process

Some hyperparameter are:  
* Learning Rate
* Number of Epochs
* Hidden Layers
* Hidden Units
* Activations Functions

## Classes and lables
input data field names = classes  
output field name = labels

# Activation function
Activation functions are functions that are applied to the weighed sum of a neuron. 
types:  

-> sigmoid:  
![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)  


outputs the input values (+ve or -ve ) to a value between the range of 0 to 1



-> tanh:   
![tanh](https://upload.wikimedia.org/wikipedia/commons/7/76/Sinh_cosh_tanh.svg)  


outputs the input values (+ve or -ve) to a value between the range of -1 to 1
better choice than sigmoid because of larger range




-> relu:  
![relu](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)  

mostly used for image data
-ve values = 0
+ve values = +ve values

-> Softmax:  
![softmax](https://www.i2tutorials.com/wp-content/uploads/2019/09/Deep-learning-27-i2tutorials.png)
Converts the input values into probabilistic values which sum up to 1
Used mostly in CNNs and multiple classes predictions

Difference between softmax and sigmoid
* sigmoid is special case of softmax where the number of classes are only 2.
* Softmax is used mostly for CNN and multiple classes predictions
* Sigmoid is used for regression based predictions*

Problem with Relu  
Relu give raise to a problem called leaky relu.
To tackel this leaky relu is used. Here the -ve values are not assigned 0 value but assigned a small -ve value

![relu_leaky](https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png)


Backpropagation
Backpropagation is the fundemental algorithm behind training neural networks. It is what changes the weights and biases of our network. This method calculates the gradient of the error function with respect to the neural network's weights.


# Loss functions
The loss function caluculates the distance between the obtained value and the actual value. 
The loss function is caluculated by choosing one of the types. The loss is caluculated for each of the obtained value and the mean of all the obtined losses is taken.

types:  
* Regression Loss Functions
    1. Mean Squared Error Loss
    2. Mean Squared Logarithmic Error Loss
    3. Mean Absolute Error Loss
* Binary Classification Loss Functions
    1. Binary Cross-Entropy
    2. Hinge Loss
    3. Squared Hinge Loss
* Multi-Class Classification Loss Functions
    1. Multi-Class Cross-Entropy Loss
    2. Sparse Multiclass Cross-Entropy Loss
    3. Kullback Leibler Divergence Loss

## Choosing a loss function and activation function
**numerical data**
*numerical based predictions*  

**Categorical: Predicting a binary outcome**  
outcome is yes or no

*fraud or not*

**Categorical: Predicting a single label from multiple classes**  
a matrix of probality values of the classes is generated by softmax. The output is a matrix of all lables with the highest probability lable as 1 and others 0  

*predicting email subjects*  

**Categorical: Predicting multiple labels from multiple classes**
a matrix of probality values of the classes is generated by sigmoid.  If 1 appears in the output, the category it corresponds to is present in the data, else a 0 appears.  

*recognising multiple objects in an image*

| Problem Type   | Output type                                                   | Activation function | Loss function        |
|----------------|---------------------------------------------------------------|---------------------|----------------------|
| Regression     | numerical data                                                | Tanh                | MSE                  |
| Classification | Categorical: Predicting a binary outcome                      | Sigmoid             | Binary cross entropy |
| Classification | Categorical: Predicting a single label from multiple classes  | Softmax             | Cross entropy        |
| Classification | Categorical: Predicting multiple labels from multiple classes | Sigmoid             | Binary cross entropy |


# Optimizers
They are the most essential part of a machine learing model. An optimizer helps in finding the correct weight and bias value for the model

## Gradient decent
![Gradient decent](https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png)  
Closely related to Backpropagation 
Weights and biases startoff with a random value and move closer to the required values over the course of the trainig period.

what is vanishing gradient?  
Found normally during the training process using gradeint decent method. In a normal training process the weights and biases are updated after every training itteration. Sometimes the wieghts and biases are no longer getting updated and athe model stops learing this is called vanishing gradient. The accuracy of the model is largly effected due to this problem.

## Adam
Mainly used in CNNs
used instead of gradient decent
mixer of Autograd and RMSprop

## Feed forward Nebural network
![ffnn](https://images.deepai.org/django-summernote/2019-06-06/5c17d9c2-0ad4-474c-be8d-d6ae9b094e74.png)
## CNN
![CNN](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)

## Steps in CNN
* convert image to grey scale
* convolution
* Maxpooling
* convolution
* maxpooling
* flatten
* Relu activation
* sigmoid for prediction

**Converting to Grey scale**
* images have mulitple dimensions (height, width, (R,G,B))
* grey scale converts image to colored image to grey scale image which has dim as height, width, black, white
* Increases the training speed

**Kernal matrix**

Matrix with fixed dimensions which parses over the image converting a big matrix into smaller matrices of the kernal matrix size

**convolution**

![conv](https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif)
Using the kernal matrix helps in representing the image in its simplest form

**Maxpooling**

![maxpooling](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)
The image after being ran through the convolution layer is then maxpooled which then picks only the highest pixel value from the each sub matrices fromed after convolution, others are concidered as noise


**Flattening**
converting X x Y matrix to Z x 1 matrix


## LSTM
![LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)

Stands for long short term memory  
Best for time series based data
Uses feedback mechanism to avoid vanishing gradient